{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aerial-cactus-identification.zip', 'data']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('.\\\\Desktop\\\\tensorflow\\\\Cactus_identification')\n",
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('data')\n",
    "os.mkdir('data\\\\train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('aerial-cactus-identification.zip', 'r') as file:\n",
    "    \n",
    "    file.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_submission.csv', 'test.zip', 'train', 'train.csv', 'train.zip']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('data\\\\train.zip', 'r') as file:\n",
    "    \n",
    "    file.extractall('data\\\\train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('data\\\\test')\n",
    "with ZipFile('data\\\\test.zip', 'r') as file:\n",
    "    \n",
    "    file.extractall('data\\\\test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_cactus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004be2cfeaba1c0361d39e2b000257b.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000c8a36845c0208e833c79c1bffedd1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000d1e9a533f62e55c289303b072733d.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011485b40695e9138e92d0b3fb55128.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014d7a11e90b62848904c1418fc8cf2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17495</th>\n",
       "      <td>ffede47a74e47a5930f81c0b6896479e.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17496</th>\n",
       "      <td>ffef6382a50d23251d4bc05519c91037.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17497</th>\n",
       "      <td>fff059ecc91b30be5745e8b81111dc7b.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17498</th>\n",
       "      <td>fff43acb3b7a23edcc4ae937be2b7522.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17499</th>\n",
       "      <td>fffd9e9b990eba07c836745d8aef1a3a.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  has_cactus\n",
       "0      0004be2cfeaba1c0361d39e2b000257b.jpg           1\n",
       "1      000c8a36845c0208e833c79c1bffedd1.jpg           1\n",
       "2      000d1e9a533f62e55c289303b072733d.jpg           1\n",
       "3      0011485b40695e9138e92d0b3fb55128.jpg           1\n",
       "4      0014d7a11e90b62848904c1418fc8cf2.jpg           1\n",
       "...                                     ...         ...\n",
       "17495  ffede47a74e47a5930f81c0b6896479e.jpg           0\n",
       "17496  ffef6382a50d23251d4bc05519c91037.jpg           1\n",
       "17497  fff059ecc91b30be5745e8b81111dc7b.jpg           1\n",
       "17498  fff43acb3b7a23edcc4ae937be2b7522.jpg           0\n",
       "17499  fffd9e9b990eba07c836745d8aef1a3a.jpg           1\n",
       "\n",
       "[17500 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_info = pd.read_csv('data\\\\train.csv')\n",
    "train_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = train_info['id'].values\n",
    "train_label = train_info['has_cactus'].values\n",
    "\n",
    "def load_data(source, data_id, size, train_label):\n",
    "    \n",
    "    data = []\n",
    "    n = 0\n",
    "    \n",
    "    for file in data_id:\n",
    "        \n",
    "        img = os.path.join(source, file)\n",
    "        img_raw = cv2.imread(img, cv2.COLOR_BGR2RGB)\n",
    "        img_array = cv2.resize(img_raw, (size, size))\n",
    "        data.append([img_array, train_label[n]])\n",
    "        n += 1\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = load_data('data\\\\train\\\\train', train_id, 128, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = np.array(img_data)\n",
    "random.shuffle(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_img_label(data):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for feature, label in data:\n",
    "        x.append(feature)\n",
    "        y.append(label)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = seperate_img_label(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17500, 128, 128, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = np.array(x), np.array(y)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  10500\n",
      "Number of validation samples:  7000\n"
     ]
    }
   ],
   "source": [
    "shuffle_idx = np.random.randint(0, len(img_data), size = (len(img_data)))\n",
    "split_point = math.ceil(0.6*len(shuffle_idx))\n",
    "train_x, train_y = x[0:split_point], y[0:split_point]\n",
    "val_x, val_y = x[split_point:], y[split_point:]\n",
    "print('Number of training samples: ', train_x.shape[0])\n",
    "print('Number of validation samples: ', val_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10500, 128, 128, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way (probably better way when using tensorflow) to preprocess the data is to use the keras.preprocessing.image.ImageDataGenerator.flow_from_dataframe API.\n",
    "\n",
    "-One thing worth noticing is that flow_from_dataframe requires that the target column must be string formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train_df = train_info.copy()\n",
    "gen_train_df['has_cactus'] = gen_train_df['has_cactus'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10500 validated image filenames belonging to 2 classes.\n",
      "Found 7000 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale = 1/255.)\n",
    "shuffle_idx = np.random.randint(0, len(img_data), size = (len(img_data)))\n",
    "split_point = math.ceil(0.6*len(shuffle_idx))\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(dataframe = gen_train_df[:split_point], directory = 'data\\\\train\\\\train', \n",
    "                                        x_col = 'id',\n",
    "                                       y_col = 'has_cactus', \n",
    "                                        class_mode = 'binary', batch_size = 50, shuffle = True,\n",
    "                                 target_size = (128, 128))\n",
    "\n",
    "val_gen = datagen.flow_from_dataframe(dataframe = gen_train_df[split_point:], directory = 'data\\\\train\\\\train', \n",
    "                                        x_col = 'id',\n",
    "                                       y_col = 'has_cactus', \n",
    "                                        class_mode = 'binary', batch_size = 20, shuffle = True,\n",
    "                                 target_size = (128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_gen:\n",
    "    print(batch[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Inception model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea is to use different type of convolutions models running in parallel, then concatenate all outputs together to generate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape = (train_x.shape[1], train_x.shape[2], train_x.shape[3]))\n",
    "\n",
    "channel = Conv2D(64, 1, strides = 2, padding = 'same', activation = 'relu')(x)\n",
    "\n",
    "channel_by_scan = Conv2D(32, 1, strides = 1, padding = 'same', activation = 'relu')(x)\n",
    "channel_by_scan = Conv2D(64, (3, 3), strides = 2, padding = 'same', activation = 'relu')(channel_by_scan)\n",
    "channel_by_scan = BatchNormalization()(channel_by_scan)\n",
    "\n",
    "spatial_capture = AveragePooling2D(3, strides = 2, padding = 'same')(x)\n",
    "spatial_capture = Conv2D(64, (3, 3), strides = 1, padding = 'same', activation = 'relu')(spatial_capture)\n",
    "spatial_capture = BatchNormalization()(spatial_capture)\n",
    "\n",
    "pattern_capture = Conv2D(32, 1, strides = 1, padding = 'same', activation = 'relu')(x)\n",
    "pattern_capture = Conv2D(64, (3, 3), strides = 1, padding = 'same', activation = 'relu')(pattern_capture)\n",
    "pattern_capture = Conv2D(64, (3, 3), strides = 2, padding = 'same', activation = 'relu')(pattern_capture)\n",
    "pattern_capture = BatchNormalization()(pattern_capture)\n",
    "\n",
    "concated = concatenate([channel, channel_by_scan, spatial_capture, pattern_capture], axis = -1)\n",
    "\n",
    "flat = Flatten()(concated)\n",
    "\n",
    "dense = Dense(128, activation = 'relu')(flat)\n",
    "dense = Dropout(0.5)(dense)\n",
    "output = Dense(1, activation = 'sigmoid')(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 128, 128, 32) 128         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 128, 128, 32) 128         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 64, 64, 3)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 128, 128, 64) 18496       conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 64, 64, 64)   18496       conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 64, 64, 64)   1792        average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 64, 64, 64)   256         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 64, 64, 64)   256         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 64, 64, 64)   256         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, 64, 64)   256         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 256)  0           conv2d_47[0][0]                  \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "                                                                 batch_normalization_25[0][0]     \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 1048576)      0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 128)          134217856   flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1)            129         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 134,294,977\n",
      "Trainable params: 134,294,593\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(x, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10500 samples, validate on 7000 samples\n",
      "Epoch 1/80\n",
      "10500/10500 [==============================] - 44s 4ms/sample - loss: 0.2819 - acc: 0.8963 - val_loss: 0.3318 - val_acc: 0.8491\n",
      "Epoch 2/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.1426 - acc: 0.9450 - val_loss: 0.2122 - val_acc: 0.9209\n",
      "Epoch 3/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.1051 - acc: 0.9623 - val_loss: 0.1105 - val_acc: 0.9650\n",
      "Epoch 4/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0760 - acc: 0.9720 - val_loss: 0.1165 - val_acc: 0.9621\n",
      "Epoch 5/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0660 - acc: 0.9768 - val_loss: 0.1042 - val_acc: 0.9664\n",
      "Epoch 6/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0591 - acc: 0.9795 - val_loss: 0.1173 - val_acc: 0.9651\n",
      "Epoch 7/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0501 - acc: 0.9833 - val_loss: 0.1057 - val_acc: 0.9677\n",
      "Epoch 8/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0362 - acc: 0.9886 - val_loss: 0.1033 - val_acc: 0.9674\n",
      "Epoch 9/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0314 - acc: 0.9897 - val_loss: 0.1011 - val_acc: 0.9691\n",
      "Epoch 10/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0389 - acc: 0.9879 - val_loss: 0.0892 - val_acc: 0.9733\n",
      "Epoch 11/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0262 - acc: 0.9918 - val_loss: 0.1251 - val_acc: 0.9636\n",
      "Epoch 12/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0198 - acc: 0.9944 - val_loss: 0.1108 - val_acc: 0.9683\n",
      "Epoch 13/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0219 - acc: 0.9934 - val_loss: 0.1023 - val_acc: 0.9731\n",
      "Epoch 14/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0187 - acc: 0.9944 - val_loss: 0.1004 - val_acc: 0.9747\n",
      "Epoch 15/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0164 - acc: 0.9949 - val_loss: 0.1055 - val_acc: 0.9739\n",
      "Epoch 16/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0142 - acc: 0.9951 - val_loss: 0.1274 - val_acc: 0.9734\n",
      "Epoch 17/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0141 - acc: 0.9960 - val_loss: 0.1074 - val_acc: 0.9729\n",
      "Epoch 18/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0117 - acc: 0.9964 - val_loss: 0.1014 - val_acc: 0.9749\n",
      "Epoch 19/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0174 - acc: 0.9939 - val_loss: 0.1132 - val_acc: 0.9723\n",
      "Epoch 20/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0088 - acc: 0.9976 - val_loss: 0.1193 - val_acc: 0.9750\n",
      "Epoch 21/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 0.1141 - val_acc: 0.9750\n",
      "Epoch 22/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0119 - acc: 0.9964 - val_loss: 0.1151 - val_acc: 0.9706\n",
      "Epoch 23/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0111 - acc: 0.9972 - val_loss: 0.1054 - val_acc: 0.9751\n",
      "Epoch 24/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0071 - acc: 0.9983 - val_loss: 0.1087 - val_acc: 0.9767\n",
      "Epoch 25/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.1021 - val_acc: 0.9757\n",
      "Epoch 26/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0072 - acc: 0.9978 - val_loss: 0.1085 - val_acc: 0.9751\n",
      "Epoch 27/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0068 - acc: 0.9978 - val_loss: 0.1201 - val_acc: 0.9751\n",
      "Epoch 28/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0054 - acc: 0.9990 - val_loss: 0.1208 - val_acc: 0.9749\n",
      "Epoch 29/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0074 - acc: 0.9977 - val_loss: 0.1187 - val_acc: 0.9746\n",
      "Epoch 30/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0059 - acc: 0.9984 - val_loss: 0.1121 - val_acc: 0.9759\n",
      "Epoch 31/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0055 - acc: 0.9984 - val_loss: 0.1129 - val_acc: 0.9757\n",
      "Epoch 32/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.1178 - val_acc: 0.9741\n",
      "Epoch 33/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0040 - acc: 0.9990 - val_loss: 0.1224 - val_acc: 0.9717\n",
      "Epoch 34/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0057 - acc: 0.9981 - val_loss: 0.1470 - val_acc: 0.9687\n",
      "Epoch 35/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0093 - acc: 0.9972 - val_loss: 0.1178 - val_acc: 0.9757\n",
      "Epoch 36/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0029 - acc: 0.9992 - val_loss: 0.1203 - val_acc: 0.9751\n",
      "Epoch 37/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.1217 - val_acc: 0.9740\n",
      "Epoch 38/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0058 - acc: 0.9985 - val_loss: 0.1405 - val_acc: 0.9699\n",
      "Epoch 39/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0052 - acc: 0.9983 - val_loss: 0.1421 - val_acc: 0.9707\n",
      "Epoch 40/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0064 - acc: 0.9979 - val_loss: 0.1250 - val_acc: 0.9751\n",
      "Epoch 41/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0039 - acc: 0.9991 - val_loss: 0.1252 - val_acc: 0.9757\n",
      "Epoch 42/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0031 - acc: 0.9989 - val_loss: 0.1173 - val_acc: 0.9757\n",
      "Epoch 43/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.1393 - val_acc: 0.9711\n",
      "Epoch 44/80\n",
      "10500/10500 [==============================] - 44s 4ms/sample - loss: 0.0014 - acc: 0.9997 - val_loss: 0.1298 - val_acc: 0.9756\n",
      "Epoch 45/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0018 - acc: 0.9995 - val_loss: 0.1416 - val_acc: 0.9720\n",
      "Epoch 46/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1310 - val_acc: 0.9747\n",
      "Epoch 47/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 9.6842e-04 - acc: 0.9997 - val_loss: 0.1317 - val_acc: 0.9760\n",
      "Epoch 48/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0053 - acc: 0.9985 - val_loss: 0.1233 - val_acc: 0.9743\n",
      "Epoch 49/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0023 - acc: 0.9992 - val_loss: 0.1272 - val_acc: 0.9756\n",
      "Epoch 50/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0026 - acc: 0.9994 - val_loss: 0.1517 - val_acc: 0.9724\n",
      "Epoch 51/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0021 - acc: 0.9994 - val_loss: 0.1393 - val_acc: 0.9750\n",
      "Epoch 52/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1248 - val_acc: 0.9770\n",
      "Epoch 53/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0030 - acc: 0.9991 - val_loss: 0.1408 - val_acc: 0.9743\n",
      "Epoch 54/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0031 - acc: 0.9990 - val_loss: 0.1502 - val_acc: 0.9746\n",
      "Epoch 55/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0030 - acc: 0.9989 - val_loss: 0.1487 - val_acc: 0.9737\n",
      "Epoch 56/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0038 - acc: 0.9988 - val_loss: 0.1480 - val_acc: 0.9741\n",
      "Epoch 57/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0018 - acc: 0.9993 - val_loss: 0.1358 - val_acc: 0.9746\n",
      "Epoch 58/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0027 - acc: 0.9991 - val_loss: 0.1257 - val_acc: 0.9756\n",
      "Epoch 59/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0018 - acc: 0.9992 - val_loss: 0.1424 - val_acc: 0.9733\n",
      "Epoch 60/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1947 - val_acc: 0.9643\n",
      "Epoch 61/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0013 - acc: 0.9996 - val_loss: 0.1507 - val_acc: 0.9743\n",
      "Epoch 62/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 8.5374e-04 - acc: 0.9999 - val_loss: 0.1540 - val_acc: 0.9759\n",
      "Epoch 63/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1552 - val_acc: 0.9751\n",
      "Epoch 64/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0021 - acc: 0.9993 - val_loss: 0.1412 - val_acc: 0.9746\n",
      "Epoch 65/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.1526 - val_acc: 0.9746\n",
      "Epoch 66/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.1658 - val_acc: 0.9704\n",
      "Epoch 67/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0016 - acc: 0.9995 - val_loss: 0.1469 - val_acc: 0.9730\n",
      "Epoch 68/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0013 - acc: 0.9995 - val_loss: 0.1795 - val_acc: 0.9750\n",
      "Epoch 69/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 7.2811e-04 - acc: 0.9999 - val_loss: 0.1658 - val_acc: 0.9736\n",
      "Epoch 70/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 8.6915e-04 - acc: 0.9997 - val_loss: 0.1534 - val_acc: 0.9747\n",
      "Epoch 71/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0037 - acc: 0.9988 - val_loss: 0.1529 - val_acc: 0.9760\n",
      "Epoch 72/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0011 - acc: 0.9996 - val_loss: 0.1594 - val_acc: 0.9744\n",
      "Epoch 73/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0021 - acc: 0.9991 - val_loss: 0.1428 - val_acc: 0.9746\n",
      "Epoch 74/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 6.2553e-04 - acc: 1.0000 - val_loss: 0.1548 - val_acc: 0.9737\n",
      "Epoch 75/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 8.4828e-04 - acc: 0.9998 - val_loss: 0.1478 - val_acc: 0.9743\n",
      "Epoch 76/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 2.7710e-04 - acc: 1.0000 - val_loss: 0.1639 - val_acc: 0.9741\n",
      "Epoch 77/80\n",
      "10500/10500 [==============================] - 43s 4ms/sample - loss: 0.0036 - acc: 0.9986 - val_loss: 0.1483 - val_acc: 0.9747\n",
      "Epoch 78/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 4.3162e-04 - acc: 1.0000 - val_loss: 0.1630 - val_acc: 0.9754\n",
      "Epoch 79/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0026 - acc: 0.9991 - val_loss: 0.1339 - val_acc: 0.9744\n",
      "Epoch 80/80\n",
      "10500/10500 [==============================] - 42s 4ms/sample - loss: 0.0012 - acc: 0.9995 - val_loss: 0.1568 - val_acc: 0.9766\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x, train_y, batch_size = 32, epochs = 80, validation_data = (val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial recognization model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this model can capture the difference of distribution between a cactus infected picture and a cactus free picture.\n",
    "Without using the pooling layer, this model, inspired by DCGAN model discriminator idea (learn the difference between distributions of different classes), should be able to learn the distribution's differences between a cactus-infected picture and a cactus-free picture, although the assumption that there is a difference between two classes, is not verified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_dist_model = Sequential()\n",
    "\n",
    "spatial_dist_model.add(Conv2D(64, (1, 1), padding = 'same', input_shape = (train_x.shape[1], train_x.shape[2], train_x.shape[3]), activation = 'relu'))\n",
    "spatial_dist_model.add(BatchNormalization()) \n",
    "\n",
    "spatial_dist_model.add(Conv2D(64, (3, 3), padding = 'same', input_shape = (train_x.shape[1], train_x.shape[2], train_x.shape[3]), activation = 'relu'))\n",
    "spatial_dist_model.add(BatchNormalization())\n",
    "\n",
    "spatial_dist_model.add(Conv2D(128, (3, 3), padding = 'same', activation = 'relu'))\n",
    "spatial_dist_model.add(BatchNormalization())\n",
    "\n",
    "spatial_dist_model.add(Conv2D(128, (3, 3), padding = 'same', activation = 'relu'))\n",
    "spatial_dist_model.add(BatchNormalization())\n",
    "\n",
    "spatial_dist_model.add(Flatten())\n",
    "spatial_dist_model.add(Dropout(0.5))\n",
    "\n",
    "spatial_dist_model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_dist_model.compile(optimizer = tf.keras.optimizers.Adam(3e-5), loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10500 samples, validate on 7000 samples\n",
      "Epoch 1/50\n",
      "10500/10500 [==============================] - 82s 8ms/sample - loss: 0.2641 - acc: 0.9544 - val_loss: 0.3040 - val_acc: 0.9477\n",
      "Epoch 2/50\n",
      "10500/10500 [==============================] - 80s 8ms/sample - loss: 0.0991 - acc: 0.9829 - val_loss: 0.3229 - val_acc: 0.9699\n",
      "Epoch 3/50\n",
      "10500/10500 [==============================] - 80s 8ms/sample - loss: 0.0612 - acc: 0.9897 - val_loss: 0.4599 - val_acc: 0.9634\n",
      "Epoch 4/50\n",
      "10500/10500 [==============================] - 80s 8ms/sample - loss: 0.0682 - acc: 0.9904 - val_loss: 0.4873 - val_acc: 0.9511\n",
      "Epoch 5/50\n",
      "10500/10500 [==============================] - 80s 8ms/sample - loss: 0.0686 - acc: 0.9904 - val_loss: 0.4613 - val_acc: 0.9719\n",
      "Epoch 6/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0468 - acc: 0.9939 - val_loss: 0.4897 - val_acc: 0.9690\n",
      "Epoch 7/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0448 - acc: 0.9944 - val_loss: 0.4105 - val_acc: 0.9761\n",
      "Epoch 8/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0468 - acc: 0.9945 - val_loss: 0.4640 - val_acc: 0.9750\n",
      "Epoch 9/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0274 - acc: 0.9966 - val_loss: 0.6747 - val_acc: 0.9720\n",
      "Epoch 10/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0438 - acc: 0.9954 - val_loss: 0.5544 - val_acc: 0.9739\n",
      "Epoch 11/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0237 - acc: 0.9962 - val_loss: 0.5908 - val_acc: 0.9770\n",
      "Epoch 12/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0459 - acc: 0.9952 - val_loss: 0.5796 - val_acc: 0.9774\n",
      "Epoch 13/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0418 - acc: 0.9969 - val_loss: 0.6731 - val_acc: 0.9761\n",
      "Epoch 14/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0263 - acc: 0.9971 - val_loss: 0.6327 - val_acc: 0.9747\n",
      "Epoch 15/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0800 - acc: 0.9953 - val_loss: 0.8046 - val_acc: 0.9719\n",
      "Epoch 16/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0280 - acc: 0.9972 - val_loss: 0.6527 - val_acc: 0.9777\n",
      "Epoch 17/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0190 - acc: 0.9978 - val_loss: 0.8233 - val_acc: 0.9759\n",
      "Epoch 18/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0334 - acc: 0.9971 - val_loss: 0.8353 - val_acc: 0.9761\n",
      "Epoch 19/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0376 - acc: 0.9968 - val_loss: 0.6643 - val_acc: 0.9754\n",
      "Epoch 20/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0312 - acc: 0.9973 - val_loss: 1.0028 - val_acc: 0.9706\n",
      "Epoch 21/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0217 - acc: 0.9988 - val_loss: 0.7441 - val_acc: 0.9747\n",
      "Epoch 22/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0247 - acc: 0.9980 - val_loss: 1.0277 - val_acc: 0.9706\n",
      "Epoch 23/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0196 - acc: 0.9979 - val_loss: 0.7688 - val_acc: 0.9784\n",
      "Epoch 24/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0363 - acc: 0.9972 - val_loss: 1.0058 - val_acc: 0.9751\n",
      "Epoch 25/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0383 - acc: 0.9978 - val_loss: 0.7831 - val_acc: 0.9766\n",
      "Epoch 26/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0261 - acc: 0.9978 - val_loss: 0.9580 - val_acc: 0.9719\n",
      "Epoch 27/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0348 - acc: 0.9979 - val_loss: 0.9066 - val_acc: 0.9754\n",
      "Epoch 28/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0165 - acc: 0.9989 - val_loss: 0.8535 - val_acc: 0.9794\n",
      "Epoch 29/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0180 - acc: 0.9987 - val_loss: 0.8246 - val_acc: 0.9773\n",
      "Epoch 30/50\n",
      "10500/10500 [==============================] - 82s 8ms/sample - loss: 0.0192 - acc: 0.9985 - val_loss: 0.9354 - val_acc: 0.9766\n",
      "Epoch 31/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0097 - acc: 0.9990 - val_loss: 0.8732 - val_acc: 0.9749\n",
      "Epoch 32/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0161 - acc: 0.9990 - val_loss: 1.0037 - val_acc: 0.9723\n",
      "Epoch 33/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0196 - acc: 0.9984 - val_loss: 0.9698 - val_acc: 0.9753\n",
      "Epoch 34/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0168 - acc: 0.9984 - val_loss: 1.0535 - val_acc: 0.9746\n",
      "Epoch 35/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0260 - acc: 0.9983 - val_loss: 1.0119 - val_acc: 0.9770\n",
      "Epoch 36/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0248 - acc: 0.9982 - val_loss: 0.9295 - val_acc: 0.9771\n",
      "Epoch 37/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0230 - acc: 0.9983 - val_loss: 1.0408 - val_acc: 0.9739\n",
      "Epoch 38/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0052 - acc: 0.9992 - val_loss: 1.3293 - val_acc: 0.9691\n",
      "Epoch 39/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0055 - acc: 0.9993 - val_loss: 1.0023 - val_acc: 0.9760\n",
      "Epoch 40/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0155 - acc: 0.9983 - val_loss: 1.1061 - val_acc: 0.9783\n",
      "Epoch 41/50\n",
      "10500/10500 [==============================] - 80s 8ms/sample - loss: 0.0082 - acc: 0.9991 - val_loss: 1.1984 - val_acc: 0.9754\n",
      "Epoch 42/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0076 - acc: 0.9990 - val_loss: 1.2389 - val_acc: 0.9760\n",
      "Epoch 43/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0170 - acc: 0.9987 - val_loss: 1.2202 - val_acc: 0.9764\n",
      "Epoch 44/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0145 - acc: 0.9989 - val_loss: 1.0339 - val_acc: 0.9780\n",
      "Epoch 45/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0192 - acc: 0.9987 - val_loss: 1.0030 - val_acc: 0.9781\n",
      "Epoch 46/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0057 - acc: 0.9994 - val_loss: 1.2717 - val_acc: 0.9764\n",
      "Epoch 47/50\n",
      "10500/10500 [==============================] - 80s 8ms/sample - loss: 0.0126 - acc: 0.9990 - val_loss: 1.4479 - val_acc: 0.9727\n",
      "Epoch 48/50\n",
      "10500/10500 [==============================] - 80s 8ms/sample - loss: 0.0207 - acc: 0.9984 - val_loss: 1.1173 - val_acc: 0.9749\n",
      "Epoch 49/50\n",
      "10500/10500 [==============================] - 81s 8ms/sample - loss: 0.0110 - acc: 0.9990 - val_loss: 1.0068 - val_acc: 0.9787\n",
      "Epoch 50/50\n",
      "10500/10500 [==============================] - 80s 8ms/sample - loss: 0.0084 - acc: 0.9990 - val_loss: 1.0656 - val_acc: 0.9757\n"
     ]
    }
   ],
   "source": [
    "spatial_dist_history = spatial_dist_model.fit(train_x, train_y, batch_size = 16, epochs = 50, validation_data = (val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Conv2D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor_model = Sequential()\n",
    "\n",
    "nor_model.add(Conv2D(64, (3, 3), padding = 'same', input_shape = (train_x.shape[1], train_x.shape[2], train_x.shape[3]), activation = 'relu'))\n",
    "nor_model.add(MaxPooling2D(2))\n",
    "nor_model.add(BatchNormalization())\n",
    "\n",
    "nor_model.add(Conv2D(128, (3, 3), padding = 'same', activation = 'relu'))\n",
    "nor_model.add(MaxPooling2D(2))\n",
    "nor_model.add(BatchNormalization())\n",
    "\n",
    "nor_model.add(Conv2D(256, (3, 3), padding = 'same', activation = 'relu'))\n",
    "nor_model.add(MaxPooling2D(2))\n",
    "nor_model.add(BatchNormalization())\n",
    "\n",
    "nor_model.add(GlobalMaxPooling2D())\n",
    "nor_model.add(Dense(256))\n",
    "nor_model.add(Dropout(0.3))\n",
    "nor_model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor_model.compile(optimizer = tf.keras.optimizers.Adam(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10500 samples, validate on 7000 samples\n",
      "Epoch 1/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 0.0031 - acc: 0.9993 - val_loss: 0.0322 - val_acc: 0.9911\n",
      "Epoch 2/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 9.4867e-04 - acc: 1.0000 - val_loss: 0.0297 - val_acc: 0.9916\n",
      "Epoch 3/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 7.9466e-04 - acc: 1.0000 - val_loss: 0.0299 - val_acc: 0.9914\n",
      "Epoch 4/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0337 - val_acc: 0.9904\n",
      "Epoch 5/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0295 - val_acc: 0.9919\n",
      "Epoch 6/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 9.8679e-04 - acc: 1.0000 - val_loss: 0.0310 - val_acc: 0.9910\n",
      "Epoch 7/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 0.0013 - acc: 0.9999 - val_loss: 0.0373 - val_acc: 0.9900\n",
      "Epoch 8/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 6.6864e-04 - acc: 1.0000 - val_loss: 0.0302 - val_acc: 0.9917\n",
      "Epoch 9/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 9.7918e-04 - acc: 1.0000 - val_loss: 0.0305 - val_acc: 0.9911\n",
      "Epoch 10/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 8.7128e-04 - acc: 1.0000 - val_loss: 0.0386 - val_acc: 0.9894\n",
      "Epoch 11/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 0.0029 - acc: 0.9989 - val_loss: 0.0346 - val_acc: 0.9906\n",
      "Epoch 12/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0313 - val_acc: 0.9914\n",
      "Epoch 13/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 0.0018 - acc: 0.9998 - val_loss: 0.0326 - val_acc: 0.9903\n",
      "Epoch 14/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 6.9056e-04 - acc: 1.0000 - val_loss: 0.0336 - val_acc: 0.9914\n",
      "Epoch 15/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 6.8329e-04 - acc: 1.0000 - val_loss: 0.0304 - val_acc: 0.9917\n",
      "Epoch 16/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 5.9596e-04 - acc: 1.0000 - val_loss: 0.0311 - val_acc: 0.9916\n",
      "Epoch 17/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0316 - val_acc: 0.9910\n",
      "Epoch 18/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 9.2693e-04 - acc: 1.0000 - val_loss: 0.0430 - val_acc: 0.9889\n",
      "Epoch 19/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 0.0012 - acc: 0.9999 - val_loss: 0.0315 - val_acc: 0.9914\n",
      "Epoch 20/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 7.1130e-04 - acc: 1.0000 - val_loss: 0.0326 - val_acc: 0.9914\n",
      "Epoch 21/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0321 - val_acc: 0.9910\n",
      "Epoch 22/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 6.1903e-04 - acc: 1.0000 - val_loss: 0.0331 - val_acc: 0.9914\n",
      "Epoch 23/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 6.7628e-04 - acc: 1.0000 - val_loss: 0.0338 - val_acc: 0.9911\n",
      "Epoch 24/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 5.6714e-04 - acc: 1.0000 - val_loss: 0.0305 - val_acc: 0.9916\n",
      "Epoch 25/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 5.6093e-04 - acc: 1.0000 - val_loss: 0.0327 - val_acc: 0.9917\n",
      "Epoch 26/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 5.1554e-04 - acc: 1.0000 - val_loss: 0.0322 - val_acc: 0.9907\n",
      "Epoch 27/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 6.8513e-04 - acc: 0.9999 - val_loss: 0.0315 - val_acc: 0.9916\n",
      "Epoch 28/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 5.1709e-04 - acc: 1.0000 - val_loss: 0.0312 - val_acc: 0.9917\n",
      "Epoch 29/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 4.6331e-04 - acc: 1.0000 - val_loss: 0.0319 - val_acc: 0.9919\n",
      "Epoch 30/30\n",
      "10500/10500 [==============================] - 16s 2ms/sample - loss: 4.3974e-04 - acc: 1.0000 - val_loss: 0.0328 - val_acc: 0.9913\n"
     ]
    }
   ],
   "source": [
    "nor_history = nor_model.fit(train_x, train_y, batch_size = 32, epochs = 30, validation_data = (val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperable Conv 2D model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Conv2D model may become very computational heavy when the input images and (or) the model is big. An alternative layer is the seperable convolutional layer which first conduct depthwise convolution then performs the usual pointwise convolution on the output of the depthwise convolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure and codes for building a Separable conv 2d model is the same as building a simple Conv2D model, except that we are now calling the SeparableConv2D layer instead of the normal Conv2D.\n",
    "\n",
    "Due to the mathmetical fundamentals behind it, the SeparableConv2D is computational easier compare to the Conv2D model and thus, we can afford to build a deeper model.\n",
    "\n",
    "To speed up the training and further reduces computations, we can also add a 1 by 1 kernel layer that has less dimensions(number of filters) than that of the previous layer's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_model = Sequential()\n",
    "\n",
    "sep_model.add(SeparableConv2D(32, (3, 3), activation = 'relu'))\n",
    "sep_model.add(SeparableConv2D(64, (3, 3), activation = 'relu'))\n",
    "sep_model.add(MaxPooling2D(2))\n",
    "sep_model.add(BatchNormalization())\n",
    "\n",
    "sep_model.add(SeparableConv2D(64, (3, 3), activation = 'relu'))\n",
    "sep_model.add(SeparableConv2D(128, (3, 3), activation = 'relu'))\n",
    "sep_model.add(MaxPooling2D(2))\n",
    "sep_model.add(BatchNormalization())\n",
    "\n",
    "sep_model.add(SeparableConv2D(64, (3, 3), activation = 'relu'))\n",
    "sep_model.add(SeparableConv2D(128, (3, 3), activation = 'relu'))\n",
    "sep_model.add(MaxPooling2D(2))\n",
    "sep_model.add(BatchNormalization())\n",
    "\n",
    "sep_model.add(SeparableConv2D(32, (1, 1), activation = 'relu'))\n",
    "\n",
    "sep_model.add(Flatten())\n",
    "sep_model.add(Dense(512, activation = 'relu'))\n",
    "sep_model.add(Dropout(0.5))\n",
    "sep_model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_model.compile(optimizer = tf.keras.optimizers.Adam(lr = 5e-5), loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  2/210 [..............................] - ETA: 1:29 - loss: 0.6917 - acc: 0.7200WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0768s vs `on_train_batch_end` time: 0.3989s). Check your callbacks.\n",
      "210/210 [==============================] - 115s 546ms/step - loss: 0.3675 - acc: 0.8398 - val_loss: 0.6486 - val_acc: 0.7557\n",
      "Epoch 2/50\n",
      "210/210 [==============================] - 116s 550ms/step - loss: 0.1600 - acc: 0.9370 - val_loss: 0.4947 - val_acc: 0.7557\n",
      "Epoch 3/50\n",
      "210/210 [==============================] - 114s 542ms/step - loss: 0.1251 - acc: 0.9513 - val_loss: 0.2539 - val_acc: 0.9081\n",
      "Epoch 4/50\n",
      "210/210 [==============================] - 113s 539ms/step - loss: 0.0952 - acc: 0.9635 - val_loss: 0.1215 - val_acc: 0.9489\n",
      "Epoch 5/50\n",
      "210/210 [==============================] - 110s 525ms/step - loss: 0.0745 - acc: 0.9724 - val_loss: 0.1607 - val_acc: 0.9284\n",
      "Epoch 6/50\n",
      "210/210 [==============================] - 108s 516ms/step - loss: 0.0639 - acc: 0.9760 - val_loss: 0.0601 - val_acc: 0.9774\n",
      "Epoch 7/50\n",
      "210/210 [==============================] - 109s 519ms/step - loss: 0.0590 - acc: 0.9775 - val_loss: 0.0511 - val_acc: 0.9837\n",
      "Epoch 8/50\n",
      "210/210 [==============================] - 110s 521ms/step - loss: 0.0511 - acc: 0.9809 - val_loss: 0.0615 - val_acc: 0.9787\n",
      "Epoch 9/50\n",
      "210/210 [==============================] - 110s 523ms/step - loss: 0.0518 - acc: 0.9804 - val_loss: 0.0500 - val_acc: 0.9841\n",
      "Epoch 10/50\n",
      "210/210 [==============================] - 109s 521ms/step - loss: 0.0452 - acc: 0.9838 - val_loss: 0.1512 - val_acc: 0.9390\n",
      "Epoch 11/50\n",
      "210/210 [==============================] - 110s 525ms/step - loss: 0.0439 - acc: 0.9840 - val_loss: 0.0574 - val_acc: 0.9796\n",
      "Epoch 12/50\n",
      "210/210 [==============================] - 109s 521ms/step - loss: 0.0427 - acc: 0.9845 - val_loss: 0.0652 - val_acc: 0.9773\n",
      "Epoch 13/50\n",
      "210/210 [==============================] - 111s 527ms/step - loss: 0.0383 - acc: 0.9849 - val_loss: 0.0466 - val_acc: 0.9849\n",
      "Epoch 14/50\n",
      "210/210 [==============================] - 109s 521ms/step - loss: 0.0389 - acc: 0.9850 - val_loss: 0.0428 - val_acc: 0.9860\n",
      "Epoch 15/50\n",
      "210/210 [==============================] - 106s 505ms/step - loss: 0.0362 - acc: 0.9877 - val_loss: 0.0502 - val_acc: 0.9826\n",
      "Epoch 16/50\n",
      "210/210 [==============================] - 109s 517ms/step - loss: 0.0315 - acc: 0.9895 - val_loss: 0.0403 - val_acc: 0.9863\n",
      "Epoch 17/50\n",
      "210/210 [==============================] - 105s 500ms/step - loss: 0.0332 - acc: 0.9881 - val_loss: 0.0447 - val_acc: 0.9853\n",
      "Epoch 18/50\n",
      "210/210 [==============================] - 109s 519ms/step - loss: 0.0312 - acc: 0.9890 - val_loss: 0.0433 - val_acc: 0.9863\n",
      "Epoch 19/50\n",
      "210/210 [==============================] - 109s 519ms/step - loss: 0.0298 - acc: 0.9891 - val_loss: 0.0494 - val_acc: 0.9826\n",
      "Epoch 20/50\n",
      "210/210 [==============================] - 109s 519ms/step - loss: 0.0271 - acc: 0.9911 - val_loss: 0.0441 - val_acc: 0.9850\n",
      "Epoch 21/50\n",
      "210/210 [==============================] - 109s 517ms/step - loss: 0.0246 - acc: 0.9915 - val_loss: 0.0427 - val_acc: 0.9849\n",
      "Epoch 22/50\n",
      "210/210 [==============================] - 111s 529ms/step - loss: 0.0245 - acc: 0.9923 - val_loss: 0.0381 - val_acc: 0.9876\n",
      "Epoch 23/50\n",
      "210/210 [==============================] - 114s 541ms/step - loss: 0.0240 - acc: 0.9915 - val_loss: 0.0612 - val_acc: 0.9789\n",
      "Epoch 24/50\n",
      "210/210 [==============================] - 114s 543ms/step - loss: 0.0225 - acc: 0.9923 - val_loss: 0.0451 - val_acc: 0.9844\n",
      "Epoch 25/50\n",
      "210/210 [==============================] - 111s 530ms/step - loss: 0.0222 - acc: 0.9923 - val_loss: 0.0822 - val_acc: 0.9736\n",
      "Epoch 26/50\n",
      "210/210 [==============================] - 117s 558ms/step - loss: 0.0241 - acc: 0.9911 - val_loss: 0.0470 - val_acc: 0.9837\n",
      "Epoch 27/50\n",
      "210/210 [==============================] - 118s 560ms/step - loss: 0.0214 - acc: 0.9923 - val_loss: 0.0367 - val_acc: 0.9884\n",
      "Epoch 28/50\n",
      "210/210 [==============================] - 111s 528ms/step - loss: 0.0172 - acc: 0.9943 - val_loss: 0.0348 - val_acc: 0.9893\n",
      "Epoch 29/50\n",
      "210/210 [==============================] - 110s 525ms/step - loss: 0.0179 - acc: 0.9936 - val_loss: 0.0348 - val_acc: 0.9890\n",
      "Epoch 30/50\n",
      "210/210 [==============================] - 106s 506ms/step - loss: 0.0158 - acc: 0.9947 - val_loss: 0.0321 - val_acc: 0.9903\n",
      "Epoch 31/50\n",
      "210/210 [==============================] - 109s 521ms/step - loss: 0.0134 - acc: 0.9960 - val_loss: 0.0388 - val_acc: 0.9876\n",
      "Epoch 32/50\n",
      "210/210 [==============================] - 109s 519ms/step - loss: 0.0126 - acc: 0.9964 - val_loss: 0.0307 - val_acc: 0.9901\n",
      "Epoch 33/50\n",
      "210/210 [==============================] - 105s 500ms/step - loss: 0.0153 - acc: 0.9952 - val_loss: 0.0395 - val_acc: 0.9860\n",
      "Epoch 34/50\n",
      "210/210 [==============================] - 109s 519ms/step - loss: 0.0120 - acc: 0.9969 - val_loss: 0.0339 - val_acc: 0.9890\n",
      "Epoch 35/50\n",
      "210/210 [==============================] - 109s 521ms/step - loss: 0.0126 - acc: 0.9955 - val_loss: 0.1047 - val_acc: 0.9631\n",
      "Epoch 36/50\n",
      "210/210 [==============================] - 109s 517ms/step - loss: 0.0124 - acc: 0.9961 - val_loss: 0.0371 - val_acc: 0.9881\n",
      "Epoch 37/50\n",
      "210/210 [==============================] - 108s 516ms/step - loss: 0.0104 - acc: 0.9972 - val_loss: 0.0470 - val_acc: 0.9839\n",
      "Epoch 38/50\n",
      "210/210 [==============================] - 113s 536ms/step - loss: 0.0116 - acc: 0.9961 - val_loss: 0.0429 - val_acc: 0.9866\n",
      "Epoch 39/50\n",
      "210/210 [==============================] - 113s 538ms/step - loss: 0.0090 - acc: 0.9975 - val_loss: 0.0376 - val_acc: 0.9880\n",
      "Epoch 40/50\n",
      "210/210 [==============================] - 114s 541ms/step - loss: 0.0089 - acc: 0.9972 - val_loss: 0.0609 - val_acc: 0.9817\n",
      "Epoch 41/50\n",
      "210/210 [==============================] - 112s 534ms/step - loss: 0.0081 - acc: 0.9977 - val_loss: 0.0857 - val_acc: 0.9753\n",
      "Epoch 42/50\n",
      "210/210 [==============================] - 109s 519ms/step - loss: 0.0081 - acc: 0.9971 - val_loss: 0.0370 - val_acc: 0.9874\n",
      "Epoch 43/50\n",
      "210/210 [==============================] - 107s 511ms/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0335 - val_acc: 0.9901\n",
      "Epoch 44/50\n",
      "210/210 [==============================] - 109s 517ms/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0336 - val_acc: 0.9896\n",
      "Epoch 45/50\n",
      "210/210 [==============================] - 108s 514ms/step - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0351 - val_acc: 0.9894\n",
      "Epoch 46/50\n",
      "210/210 [==============================] - 69s 327ms/step - loss: 0.0078 - acc: 0.9970 - val_loss: 0.0367 - val_acc: 0.9894\n",
      "Epoch 47/50\n",
      "210/210 [==============================] - 107s 510ms/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0372 - val_acc: 0.9886\n",
      "Epoch 48/50\n",
      "210/210 [==============================] - 108s 515ms/step - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0314 - val_acc: 0.9914\n",
      "Epoch 49/50\n",
      "210/210 [==============================] - 109s 518ms/step - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0439 - val_acc: 0.9870\n",
      "Epoch 50/50\n",
      "210/210 [==============================] - 108s 514ms/step - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0701 - val_acc: 0.9819\n"
     ]
    }
   ],
   "source": [
    "sep_history = sep_model.fit(train_gen, steps_per_epoch = 210, epochs = 50, validation_data = val_gen, validation_steps = 350)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
